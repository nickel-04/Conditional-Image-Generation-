{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e94af8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import glob\n",
    "import hashlib\n",
    "import io\n",
    "import litdata\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "import tqdm\n",
    "\n",
    "from datetime import datetime\n",
    "from IPython.display import display, clear_output\n",
    "from itertools import product\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import ConcatDataset, DataLoader, random_split, WeightedRandomSampler\n",
    "from torchvision.models import vgg16\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torchvision.utils import make_grid, save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14e59e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Specify data folder\n",
    "datapath = '/projects/ec232/data/'\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define mean and std from ImageNet data statistics\n",
    "in_mean = [0.485, 0.456, 0.406]\n",
    "in_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "class ToRGBTensor:\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        return TF.to_tensor(img).expand(3, -1, -1) # Expand to 3 channels\n",
    "        \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}()\"\n",
    "\n",
    "def scores_to_tensor(scores_path):\n",
    "    scores = np.load(scores_path)\n",
    "    return torch.tensor(scores, dtype=torch.float32)    \n",
    "    \n",
    "## Updated scores_to_tensor function\n",
    "def scores_to_tensor(scores):\n",
    "    return torch.tensor(scores, dtype=torch.float32)\n",
    "\n",
    "# Redefine postprocessing / transform of data modalities\n",
    "postprocess = (                                # Create tuple for image and scores...\n",
    "    T.Compose([                                # Handles processing of the .jpg image\n",
    "        ToRGBTensor(),                         # Convert from PIL image to RGB torch.Tensor.\n",
    "        T.Resize((224, 224), antialias=True),  # Resize the image to 224, 224 for ResNet18\n",
    "        #T.Normalize(in_mean, in_std),         # Normalize image to correct mean/std.\n",
    "    ]),\n",
    "    scores_to_tensor                           # Handles processing of .scores.npy file.\n",
    ")\n",
    "\n",
    "# Reload training data with the updated transformation\n",
    "data = litdata.LITDataset(\n",
    "    'CarRecs',\n",
    "    datapath,\n",
    "    override_extensions =[ # Sets the order of the modalities:\n",
    "        'jpg', # ... load image first\n",
    "        'scores.npy' # ... load scores second\n",
    "    ],\n",
    ").map_tuple(*postprocess)\n",
    "\n",
    "# Test accessing a sample\n",
    "sample = data[0]\n",
    "print(\"Image shape:\", sample[0].shape)\n",
    "print(\"Scores tensor:\", sample[1])\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dff0d5",
   "metadata": {},
   "source": [
    "## Dataset split\n",
    "\n",
    "**Product of Scores:**\n",
    "\n",
    "Multiplying the scores of the two reviewers for each image.\n",
    "Pros:\n",
    "Prioritizes images that both reviewers gave a high score to.\n",
    "Cons:\n",
    "Low scores from either reviewer can drastically reduce the weight of an image.\n",
    "\n",
    "**Consensus-Based:**\n",
    "\n",
    "Prioritizing images where there's a smaller difference between the scores of the two reviewers (indicating consensus).\n",
    "Pros:\n",
    "Focuses on images where both reviewers have a mutual understanding or agreement.\n",
    "Cons:\n",
    "Might ignore images with extreme scores from one reviewer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d6786f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract scores for Moira and Ferdinando\n",
    "moira_scores = [item[1][0][1].item() for item in data]  # 1 represents Moira's position\n",
    "ferdinando_scores = [item[1][0][3].item() for item in data]  # 3 represents Ferdinando's position\n",
    "\n",
    "\n",
    "# Set image batch size\n",
    "batch_size = 32\n",
    "\n",
    "# 1. Product of Scores\n",
    "product_weights = [moira_score * ferdinando_score for moira_score, ferdinando_score in zip(moira_scores, ferdinando_scores)]\n",
    "\n",
    "# 2. Consensus-Based\n",
    "consensus_weights = [1 / (abs(moira_score - ferdinando_score) + 1) for moira_score, ferdinando_score in zip(moira_scores, ferdinando_scores)]\n",
    "\n",
    "# Create full dataset\n",
    "full_dataset = [(item[0], (torch.tensor(moira_scores[i]), torch.tensor(ferdinando_scores[i]))) for i, item in enumerate(data)]\n",
    "\n",
    "# Split the dataset\n",
    "train_size = int(0.9 * len(data))\n",
    "val_size = len(data) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# Extract training weights for both strategies\n",
    "train_product_weights = [product_weights[i] for i in train_dataset.indices]\n",
    "train_consensus_weights = [consensus_weights[i] for i in train_dataset.indices]\n",
    "\n",
    "# Create the WeightedRandomSampler for both strategies\n",
    "sampler_product = WeightedRandomSampler(train_product_weights, num_samples=len(train_dataset), replacement=True)\n",
    "sampler_consensus = WeightedRandomSampler(train_consensus_weights, num_samples=len(train_dataset), replacement=True)\n",
    "\n",
    "# Create data loaders for both strategies\n",
    "train_loader_product = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler_product)\n",
    "train_loader_consensus = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler_consensus)\n",
    "\n",
    "# Validation loaders remain the same for both as we don't apply weighting to a validation set\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_loader_product, train_loader_consensus\n",
    "\n",
    "train_loader = train_loader_product\n",
    "val_loader = val_loader\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7922a12b",
   "metadata": {},
   "source": [
    "## Pretrained ResNet18 VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf713ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class CVAE_ResNet18_DualEmbedding(nn.Module):\n",
    "    def __init__(self, conditional_dim=2, latent_dim=512, debug=False):\n",
    "        super(CVAE_ResNet18_DualEmbedding, self).__init__()\n",
    "        self.debug = debug\n",
    "        self.bn1 = nn.BatchNorm2d(256)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.bn4 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conditional_dim = conditional_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Load the ResNet18 model with pretrained weights\n",
    "        self.resnet18 = models.resnet18(weights='DEFAULT')\n",
    "        \n",
    "        # Extract the feature layers\n",
    "        self.features = nn.Sequential(*list(self.resnet18.children())[:-1])\n",
    "        \n",
    "        # Encoder layers\n",
    "        self.fc_mu = nn.Linear(512 + 2, self.latent_dim)\n",
    "        self.fc_logvar = nn.Linear(512 + 2, self.latent_dim)\n",
    "        \n",
    "        # Decoder layers\n",
    "        self.decoder_input = nn.Linear(self.latent_dim + 2, 512 * 7 * 7)\n",
    "        self.deconv1 = nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv4 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv5 = nn.ConvTranspose2d(32, 3, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "    def encode(self, x, c):\n",
    "        # Extract features using ResNet18\n",
    "        x = self.features(x)\n",
    "        if self.debug: print(f\"After ResNet18 features extraction: {x.shape}\")\n",
    "        x = x.view(x.size(0), 512)  # only flatten the spatial dimensions\n",
    "        if self.debug: print(f\"After reshaping: {x.shape}\")\n",
    "        \n",
    "        # Concatenate condition vectors (for Moira and Ferdinando)\n",
    "        x = torch.cat([x, c], dim=1)\n",
    "        if self.debug: print(f\"After concatenating with condition vector: {x.shape}\")\n",
    "        \n",
    "        mu = self.fc_mu(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z, c):\n",
    "        # Concatenate z with condition vectors (for Moira and Ferdinando)\n",
    "        z = torch.cat([z, c], dim=1)\n",
    "        if self.debug: print(f\"After concatenating z with condition vector: {z.shape}\")\n",
    "\n",
    "        x = F.relu(self.decoder_input(z))\n",
    "        x = x.view(x.size(0), 512, 7, 7)  # reshape to (batch, channels, height, width)\n",
    "        x = F.relu(self.bn1(self.deconv1(x)))\n",
    "        x = F.relu(self.bn2(self.deconv2(x)))\n",
    "        x = F.relu(self.bn3(self.deconv3(x)))\n",
    "        x = F.relu(self.bn4(self.deconv4(x)))\n",
    "        x = torch.sigmoid(self.deconv5(x))\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x, c):\n",
    "        if self.debug: print(f\"Input image shape: {x.shape}, Condition shape: {c.shape}\")\n",
    "        mu, logvar = self.encode(x, c)\n",
    "        if self.debug: print(f\"Encoding outputs - mu: {mu.shape}, logvar: {logvar.shape}\")\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        if self.debug: print(f\"Latent representation z shape: {z.shape}\")\n",
    "        recon_x = self.decode(z, c)\n",
    "        return recon_x, mu, logvar\n",
    "    \n",
    "cvae = CVAE_ResNet18_DualEmbedding(conditional_dim=2, latent_dim=512, debug=True).to(device)\n",
    "print(cvae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0dc3fb",
   "metadata": {},
   "source": [
    "## Initializing metrics\n",
    "\n",
    "**Importance of Losses for VAE:**\n",
    "\n",
    "**MSE (Mean Squared Error):**\n",
    "\n",
    "Importance: Fundamental for VAEs. Ensures pixel-level fidelity.\n",
    "When to Use: Always, especially in the early stages of training.\n",
    "Issues: May result in blurry reconstructions if used alone.\n",
    "\n",
    "**Perceptual Loss:**\n",
    "\n",
    "Importance: Captures high-level semantic differences between images, ensuring that the generated images are perceptually similar to the target images.\n",
    "When to Use: Once the VAE starts producing reasonable reconstructions, to refine the quality.\n",
    "Issues: Can introduce artifacts if weighted too highly.\n",
    "\n",
    "**Histogram Loss:**\n",
    "\n",
    "Importance: Ensures that the distribution of pixel intensities in the generated image matches the target, which can be important for capturing image characteristics like brightness and contrast.\n",
    "When to Use: Useful when the color distribution or intensity distribution is crucial.\n",
    "Issues: Might not be necessary if the other two losses already produce satisfactory results.\n",
    "In general, for VAEs, the primary loss is the combination of the reconstruction loss (like MSE) and the KL divergence, which ensures that the learned latent space has desirable properties. The additional perceptual and histogram losses are supplementary and help refine the image quality based on the application's requirements.\n",
    "\n",
    "To decide the importance, we need to consider the goals of our application. If pixel-level fidelity is crucial, prioritize MSE. If perceptual quality matters more (e.g., for art generation or photo enhancement), the perceptual loss becomes more important. If maintaining image characteristics like brightness or contrast is essential, then histogram loss should be emphasized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bb787d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loss function: combination of reconstruction MSE loss and KL divergence\n",
    "def mse_loss(recon_x, x, mu, logvar, beta):\n",
    "    # Mean Squared Error\n",
    "    # MSE = F.mse_loss(recon_x, x, reduction='sum') / x.numel() # Normalize by the number of pixels per batch\n",
    "    MSE = F.mse_loss(recon_x, x, reduction='sum') / x.size(0)   # Normalize by batch size\n",
    "    \n",
    "    # Kullback-Leibler divergence loss\n",
    "    KLD = (-0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())) / x.size(0)   # Normalize by batch size\n",
    "    \n",
    "    return MSE + beta * KLD, MSE, KLD, 0, 0, 0\n",
    "\n",
    "\n",
    "# Computes the VAE loss = reconstruction MS_SSIM loss + KL divergence\n",
    "from torchmetrics.image import MultiScaleStructuralSimilarityIndexMeasure\n",
    "ms_ssim_module = MultiScaleStructuralSimilarityIndexMeasure(data_range=1.0, betas=(0.0448, 0.2856, 0.3001, 0.2363, 0.1333)[:3]).to(device) # adjusted number of scales for 128*128 images\n",
    "def ms_ssim_loss(recon_x, x, mu, logvar, beta=1.0): \n",
    "    \n",
    "    # Reconstruction loss using MS-SSIM\n",
    "    ms_ssim_val = ms_ssim_module(recon_x, x)\n",
    "    ms_ssim_loss = 1 - ms_ssim_val  # 1 - MS-SSIM gives the loss\n",
    "    \n",
    "    # KL divergence\n",
    "    kld_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / x.size(0)  # Normalize by batch size\n",
    "    \n",
    "    return ms_ssim_loss + beta * kld_loss, ms_ssim_loss, kld_loss, 0, 0, 0\n",
    "\n",
    "\n",
    "# Load the VGG16 model and extract features from an intermediate layer\n",
    "class VGGPerceptualLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGPerceptualLoss, self).__init__()\n",
    "        model = vgg16(weights='DEFAULT')\n",
    "        self.features = model.features[:16]\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x_features = self.features(x)\n",
    "        y_features = self.features(y)\n",
    "        return F.mse_loss(x_features, y_features)\n",
    "\n",
    "perceptual_criterion = VGGPerceptualLoss().to(device)\n",
    "\n",
    "\n",
    "# Enhanced CVAE loss which combines MSE, Perceptual, and Histogram-based loss\n",
    "def histogram_loss(img1, img2, bins=64):\n",
    "    # Ensure images are normalized\n",
    "    # output = (output - output.min()) / (output.max() - output.min())\n",
    "    # target = (target - target.min()) / (target.max() - target.min())\n",
    "    \n",
    "    # print(f\"Output Image - Min: {img1.min().item()}, Max: {img1.max().item()}, Mean: {img1.mean().item()}, Std: {img1.std().item()}\")\n",
    "    # print(f\"Target Image - Min: {img2.min().item()}, Max: {img2.max().item()}, Mean: {img2.mean().item()}, Std: {img2.std().item()}\")\n",
    "\n",
    "    hist1 = torch.histc(img1, bins=bins, min=0, max=1)\n",
    "    hist2 = torch.histc(img2, bins=bins, min=0, max=1)\n",
    "\n",
    "    # Normalize the histograms\n",
    "    hist1 = hist1 / hist1.sum()\n",
    "    hist2 = hist2 / hist2.sum()\n",
    "    \n",
    "    def is_normalized(img):\n",
    "        min_val = img.min().item()\n",
    "        max_val = img.max().item()\n",
    "        if 0 <= min_val and max_val <= 1:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    #if not is_normalized(img1) or not is_normalized(img2):\n",
    "        #print(\"Warning: Input images to histogram_loss are not normalized but standardized to (0, 1)\")\n",
    "    \n",
    "    return F.mse_loss(hist1, hist2)\n",
    "\n",
    "def combined_cvae_loss(recon_x, x, mu, logvar, beta, alpha, theta, lamda):\n",
    "    #mse_loss = F.mse_loss(recon_x, x)\n",
    "    mse_loss = F.mse_loss(recon_x, x, reduction='sum') / x.size(0)   # Normalize by batch size\n",
    "    perceptual_loss = perceptual_criterion(recon_x, x)\n",
    "    hist_loss = histogram_loss(recon_x, x)\n",
    "    \n",
    "    # Combine the losses\n",
    "    recon_loss = alpha * mse_loss + theta * perceptual_loss + lamda * hist_loss\n",
    "    \n",
    "    # KL divergence\n",
    "    kld_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    kld_loss = kld_loss / x.size(0)  # normalize by batch size\n",
    "    \n",
    "    total_loss = recon_loss + beta * kld_loss\n",
    "    loss_dict = {\n",
    "        \"recon_loss\": recon_loss,\n",
    "        \"kld_loss\": kld_loss,\n",
    "        \"mse_loss\": mse_loss,\n",
    "        \"perceptual_loss\": perceptual_loss,\n",
    "        \"hist_loss\": hist_loss\n",
    "    }\n",
    "    \n",
    "    # Debugging print statements\n",
    "    # print(f\"Debug - recon_loss: {recon_loss.item()}, kld_loss: {kld.item()}, mse_loss: {mse_loss.item()}, perceptual_loss: {perceptual_loss.item()}, hist_loss: {hist_loss.item()}\")\n",
    "    \n",
    "    return total_loss, loss_dict\n",
    "\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(cvae.parameters(), lr=1e-3)\n",
    "\n",
    "# Check if everything is set up correctly\n",
    "mse_loss, optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b01425c",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a413e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Capture current time for folder naming.\n",
    "current_time = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "\n",
    "\n",
    "# Main function for training the model and visualizing losses.\n",
    "def train_and_visualize_losses(\n",
    "    model,\n",
    "    train_loader,                                                # DataLoader for training data\n",
    "    val_loader,                                                  # DataLoader for validation data\n",
    "    optimizer,                                                   # Optimizer for model parameters\n",
    "    scheduler,                                                   # Learning rate scheduler\n",
    "    hyperparameters,\n",
    "):\n",
    "\n",
    "    start_times = []\n",
    "    \n",
    "    # Initialize all the required parameters from hyperparameter dictionary\n",
    "    num_epochs=hyperparameters[\"num_epochs\"]                    # Total number of epochs to train the model\n",
    "    patience=hyperparameters[\"patience\"]                        # Number of epochs with no improvement after which training will be stopped\n",
    "    plateau_threshold=hyperparameters[\"plateau_threshold\"]      # Number of epochs to wait before considering it a plateau (for learning rate adjustment)\n",
    "    save_interval=hyperparameters[\"save_interval\"]              # Interval at which the model checkpoints are saved\n",
    "    learning_rate=hyperparameters[\"learning_rate\"]              # Learning rate for the optimizer\n",
    "    weight_decay=hyperparameters[\"weight_decay\"]                # Weight decay regularization to prevent model from overfitting\n",
    "    latent_dim=hyperparameters[\"latent_dim\"]                    # Dimensionality of the latent space\n",
    "    condition_dim=hyperparameters[\"condition_dim\"]              # Dimensionality of the condition vector (e.g., 2 for two scores in our case)\n",
    "    loss_type=hyperparameters[\"loss_type\"]                      # Type of the loss function ('mse', 'ms-ssim', or 'combined')\n",
    "    add_loss_on_epoch=hyperparameters[\"add_loss_on_epoch\"]      # Epoch number after which additional loss terms start getting added\n",
    "    alpha=hyperparameters[\"alpha\"]                              # Weight for the MSE term in the loss when using 'combined' loss_type\n",
    "    beta=hyperparameters[\"beta\"]                                # Weight for the KL-divergence term in the loss\n",
    "    theta=hyperparameters[\"theta\"]                              # Weight for the perceptual loss term\n",
    "    lamda=hyperparameters[\"lamda\"]                              # Weight for the histogram loss term\n",
    "    interval_perceptual=hyperparameters[\"interval_perceptual\"]  # Delay prior to introducing additional loss.\n",
    "    interval_kld=hyperparameters[\"interval_kld\"]                # Delay prior to introducing additional loss.\n",
    "    interval_hist=hyperparameters[\"interval_hist\"]              # Delay prior to introducing additional loss.\n",
    "    images_to_show=hyperparameters[\"images_to_show\"]            # Number of images to display during training for visual inspection\n",
    "    nrow=hyperparameters[\"nrow\"]                                # Number of rows when showing the images using `make_grid`\n",
    "    image_show=hyperparameters[\"image_show\"]                    # Whether to show images during training\n",
    "    debug=hyperparameters[\"debug\"]                              # If true, print out additional debug information\n",
    "    \n",
    "\n",
    "    \n",
    "    # Function to compute MD5 hash of a tensor. Used for debugging.\n",
    "    def compute_hash(tensor):\n",
    "        return hashlib.md5(tensor.cpu().numpy().tobytes()).hexdigest()\n",
    "\n",
    "    # Function to denormalize a tensor (convert from [-1, 1] to [0, 255]).\n",
    "    def denormalize(tensor, mean=in_mean, std=in_std):\n",
    "        for t, m, s in zip(tensor, mean, std):\n",
    "            t.mul_(s).add_(m)\n",
    "        return tensor\n",
    "\n",
    "    # Function to select the appropriate loss function based on the type.\n",
    "    def select_loss_function(loss_type):\n",
    "        if loss_type == 'mse':\n",
    "            return mse_loss\n",
    "        elif loss_type == 'ms-ssim':\n",
    "            return ms_ssim_loss\n",
    "        elif loss_type == 'combined':\n",
    "            return combined_cvae_loss\n",
    "        else:\n",
    "            raise ValueError(\"Invalid loss_type. Expected 'mse', 'ms-ssim', or 'combined'.\")\n",
    "\n",
    "\n",
    "    def analyze_latent_space(corr_matrix, latent_vectors):\n",
    "        # Thresholds for analysis\n",
    "        high_corr_threshold = 0.9\n",
    "        low_variance_threshold = 0.01\n",
    "\n",
    "        # Identify pairs of highly correlated dimensions\n",
    "        high_corr_pairs = []\n",
    "        for i in range(corr_matrix.shape[0]):\n",
    "            for j in range(i+1, corr_matrix.shape[1]):\n",
    "                if abs(corr_matrix[i][j]) > high_corr_threshold:\n",
    "                    high_corr_pairs.append((i, j))\n",
    "\n",
    "        # Flag latent dimensions with low variance\n",
    "        latent_variances = np.var(latent_vectors, axis=0)\n",
    "        low_variance_dims = np.where(latent_variances < low_variance_threshold)[0]\n",
    "\n",
    "        # Analysis and Interpretation\n",
    "        num_high_corr_pairs = len(high_corr_pairs)\n",
    "        num_low_variance_dims = len(low_variance_dims)\n",
    "\n",
    "        print(f\"Number of highly correlated pairs of latent dimensions: {num_high_corr_pairs}\")\n",
    "        print(f\"Number of latent dimensions with low variance: {num_low_variance_dims}\\n\")\n",
    "\n",
    "        if num_high_corr_pairs > (0.5 * corr_matrix.shape[0]):\n",
    "            print(\"A significant portion of the latent dimensions are highly correlated with each other. This might suggest redundancy in the latent space.\")\n",
    "        else:\n",
    "            print(\"The latent dimensions seem to be reasonably independent, suggesting efficient utilization of the latent space.\")\n",
    "\n",
    "        if num_low_variance_dims > (0.5 * latent_vectors.shape[1]):\n",
    "            print(\"A significant number of latent dimensions exhibit low variance, indicating potential underutilization of the latent space.\\n\\n\\n\")\n",
    "        else:\n",
    "            print(\"Most latent dimensions have reasonable variance, suggesting they might be carrying significant information.\\n\\n\\n\")\n",
    "\n",
    "        return high_corr_pairs, low_variance_dims\n",
    "\n",
    "    # Create a folder to save interim images during the training process.\n",
    "    def save_hyperparameters_to_txt(folder, hyperparameters):\n",
    "        with open(os.path.join(folder, \"hyperparameters.txt\"), \"w\") as f:\n",
    "            for key, value in hyperparameters.items():\n",
    "                f.write(f\"{key}: {value}\\n\")\n",
    "  \n",
    "    # Create a folder name based on hyperparameters.\n",
    "    folder_suffix = f\"beta_{hyperparameters['beta']}_ldim_{hyperparameters['latent_dim']}_{hyperparameters['loss_type']}_loss\"\n",
    "    images_folder = f\"results/training_images_{current_time}_{folder_suffix}\"\n",
    "    os.makedirs(images_folder, exist_ok=True)\n",
    "\n",
    "    # Save hyperparameters to a text file.\n",
    "    save_hyperparameters_to_txt(images_folder, hyperparameters)\n",
    "    \n",
    "    # Initialize loss variables.\n",
    "    epoch_mse_loss = 0.0\n",
    "    epoch_perceptual_loss = 0.0\n",
    "    epoch_hist_loss = 0.0\n",
    "    \n",
    "    mse_loss_accum = 0.0\n",
    "    perceptual_loss_accum = 0.0\n",
    "    hist_loss_accum = 0.0\n",
    "    \n",
    "    # Select the appropriate loss function.\n",
    "    print(f\"Received loss_type: '{loss_type}'\")\n",
    "    criterion = select_loss_function(loss_type)\n",
    "    \n",
    "    # Initialize losses to zero if not using the 'combined' loss.\n",
    "    if loss_type != 'combined':\n",
    "        kld_loss, perceptual_loss, hist_loss = 0, 0, 0\n",
    "        \n",
    "    # Initialize list to store latent vectors\n",
    "    latent_vectors = []\n",
    "    \n",
    "    # Store initial model weights and initialize best loss to infinity.\n",
    "    best_model_wts = model.state_dict()\n",
    "    best_loss = float('inf')\n",
    "    no_improve = 0\n",
    "    \n",
    "    # Dictionaries to store training and validation losses.\n",
    "    train_losses = {\n",
    "        'total': [], \n",
    "        'reconstruction_loss': [], \n",
    "        'kld_loss': [], \n",
    "        'mse_loss': [], \n",
    "        'perceptual_loss': [], \n",
    "        'hist_loss': []\n",
    "    }\n",
    "    val_losses = {\n",
    "        'total': [], \n",
    "        'reconstruction_loss': [], \n",
    "        'kld_loss': [], \n",
    "        'mse_loss': [], \n",
    "        'perceptual_loss': [], \n",
    "        'hist_loss': []\n",
    "    }\n",
    "    \n",
    "    # Get a fixed batch from the training set for visualization during the training phase.\n",
    "    fixed_train_iter = iter(train_loader)\n",
    "    fixed_train_batch, fixed_train_labels = next(fixed_train_iter)\n",
    "    fixed_train_batch = fixed_train_batch.to(device)\n",
    "    moira_fixed_train_scores = fixed_train_labels[0].unsqueeze(1).float().to(device)\n",
    "    ferdinando_fixed_train_scores = fixed_train_labels[1].unsqueeze(1).float().to(device)\n",
    "    fixed_train_labels = torch.cat([moira_fixed_train_scores, ferdinando_fixed_train_scores], dim=1)\n",
    "\n",
    "    fixed_data_iter = iter(val_loader)\n",
    "    fixed_val_batch, fixed_val_labels = next(fixed_data_iter)\n",
    "    fixed_val_batch = fixed_val_batch.to(device)\n",
    "    moira_fixed_val_scores = fixed_val_labels[0].unsqueeze(1).float().to(device)\n",
    "    ferdinando_fixed_val_scores = fixed_val_labels[1].unsqueeze(1).float().to(device)\n",
    "    fixed_val_labels = torch.cat([moira_fixed_val_scores, ferdinando_fixed_val_scores], dim=1)\n",
    "    \n",
    "    # Define beta values for KL divergence weight adjustment.\n",
    "    beta_values = [0.01, 0.1, 0.5, 1.0]\n",
    "    beta_update_interval = 10 * round((num_epochs // (len(beta_values) + 1)) / 10)\n",
    "    beta_index = 0\n",
    "\n",
    "    # Original values\n",
    "    original_alpha = alpha\n",
    "    original_beta = beta\n",
    "    original_theta = theta\n",
    "    original_lamda = lamda\n",
    "    \n",
    "    # Variables for monitoring validation loss and introducing additional losses.\n",
    "    prev_val_loss = float('inf')\n",
    "    plateau_count = 0\n",
    "    plateau_threshold = 10  # Number of epochs to wait before considering it a plateau.\n",
    "    \n",
    "    # Flags for the introduction of additional losses.\n",
    "    kld_introduced = False\n",
    "    perceptual_introduced = False\n",
    "    hist_introduced = False\n",
    "    \n",
    "    # Training loop.\n",
    "    for epoch in range(num_epochs):\n",
    "               \n",
    "        # Record the start time of the epoch.\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Initialize the list to store latent vectors for this epoch.\n",
    "        latent_vectors = []  \n",
    "            \n",
    "        print(f\"Epoch {epoch}/{num_epochs - 1}\")\n",
    "        print('-' * 10)\n",
    "        \n",
    "        # Re-assign the original values at the start of each epoch       \n",
    "        if kld_introduced:\n",
    "            beta = original_beta\n",
    "        if perceptual_introduced:\n",
    "            theta = original_theta\n",
    "        if hist_introduced:\n",
    "            lamda = original_lamda\n",
    "        \n",
    "        # Set beta, theta, and lamda to 0 if current epoch is less than additional_loss_on_epoch.\n",
    "        if epoch < add_loss_on_epoch:\n",
    "            alpha, beta, theta, lamda = 1, 0, 0, 0 #hyperparameters['beta']\n",
    "            can_monitor_plateau = False\n",
    "        else:\n",
    "            can_monitor_plateau = True \n",
    "        \n",
    "        # Initialize running losses.\n",
    "        running_loss = 0.0\n",
    "        running_reconstruction_loss = 0.0\n",
    "        running_kld = 0.0\n",
    "        running_mse_loss = 0.0\n",
    "        running_perceptual_loss = 0.0\n",
    "        running_hist_loss = 0.0\n",
    "        \n",
    "        # Iterate over training and validation phases.\n",
    "        for phase in ['train', 'val']:\n",
    "            \n",
    "            if phase == 'train':\n",
    "                fixed_batch = fixed_train_batch\n",
    "                fixed_labels = fixed_train_labels\n",
    "            else: # 'val' phase\n",
    "                fixed_batch = fixed_val_batch\n",
    "                fixed_labels = fixed_val_labels\n",
    "                \n",
    "            # Compute and store the hash for the fixed batch for consistency check.\n",
    "            fixed_batch_hash = compute_hash(fixed_batch)\n",
    "            \n",
    "            # Set the model to training mode during the training phase and evaluation mode during the validation phase.\n",
    "            model.train() if phase == 'train' else model.eval()\n",
    "            \n",
    "            # Select the appropriate data loader.\n",
    "            dataloader = train_loader if phase == 'train' else val_loader\n",
    "\n",
    "            # Iterate over batches.\n",
    "            for inputs, (moira_scores, ferdinando_scores) in dataloader:\n",
    "                inputs = inputs.to(device)\n",
    "                moira_scores = moira_scores.unsqueeze(1).float().to(device)\n",
    "                ferdinando_scores = ferdinando_scores.unsqueeze(1).float().to(device)\n",
    "                combined_scores = torch.cat([moira_scores, ferdinando_scores], dim=1)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass.\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    recon_batch, mu, logvar = model(inputs, combined_scores)\n",
    "                    \n",
    "                    # Store latent vectors\n",
    "                    latent_vectors.append(mu.detach().cpu().numpy())\n",
    "                    \n",
    "                    # Compute the loss.\n",
    "                    loss, loss_dict = combined_cvae_loss(recon_batch, inputs, mu, logvar, beta, alpha, theta, lamda)\n",
    "                    \n",
    "                    # Perform backward pass and optimization during the training phase.\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Update running losses.\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_reconstruction_loss += loss_dict['recon_loss'].item() * inputs.size(0)\n",
    "                running_kld += loss_dict['kld_loss'].item() * inputs.size(0)\n",
    "\n",
    "                if hyperparameters['loss_type'] == 'combined':\n",
    "                    running_mse_loss += loss_dict['mse_loss'].item() * inputs.size(0)\n",
    "                    running_perceptual_loss += loss_dict['perceptual_loss'].item() * inputs.size(0)\n",
    "                    running_hist_loss += loss_dict['hist_loss'].item() * inputs.size(0)\n",
    "\n",
    "            # Compute average losses.\n",
    "            epoch_loss = running_loss / len(dataloader.dataset)\n",
    "            epoch_reconstruction_loss = running_reconstruction_loss / len(dataloader.dataset)\n",
    "            epoch_kld = running_kld / len(dataloader.dataset)\n",
    "            \n",
    "            if hyperparameters['loss_type'] == 'combined':\n",
    "                epoch_mse_loss = running_mse_loss / len(dataloader.dataset)\n",
    "                epoch_perceptual_loss = running_perceptual_loss / len(dataloader.dataset)\n",
    "                epoch_hist_loss = running_hist_loss / len(dataloader.dataset)\n",
    "\n",
    "            # Store epoch losses.\n",
    "            if phase == 'train':\n",
    "                train_losses['mse_loss'].append(epoch_mse_loss)\n",
    "                train_losses['perceptual_loss'].append(epoch_perceptual_loss)\n",
    "                train_losses['hist_loss'].append(epoch_hist_loss)\n",
    "            else:\n",
    "                val_losses['mse_loss'].append(epoch_mse_loss)\n",
    "                val_losses['perceptual_loss'].append(epoch_perceptual_loss)\n",
    "                val_losses['hist_loss'].append(epoch_hist_loss)\n",
    "\n",
    "            # Update the scheduler based on the validation loss.\n",
    "            if phase == 'val' and scheduler:\n",
    "                scheduler.step(epoch_loss)\n",
    "\n",
    "            print(f\"{phase} losses:       \"\n",
    "                  f\"{hyperparameters['loss_type']}: {epoch_reconstruction_loss:.2f}, \"\n",
    "                  f\"mse_loss: {alpha * epoch_mse_loss:.2f}, \"\n",
    "                  f\"kld_loss: {beta * epoch_kld:.2f}, \"\n",
    "                  f\"perceptual_loss: {theta * epoch_perceptual_loss:.2f}, \"\n",
    "                  f\"histogram_loss: {lamda * epoch_hist_loss:.2f}\")\n",
    "\n",
    "\n",
    "            # Check for validation loss plateau and introduce additional losses.\n",
    "            if phase == 'val' and can_monitor_plateau:\n",
    "\n",
    "                if (abs(prev_val_loss - epoch_loss) / prev_val_loss) < 0.01:  # small threshold for considering it a plateau.\n",
    "                    plateau_count += 1\n",
    "                else:\n",
    "                    plateau_count = 0\n",
    "\n",
    "                # Introduce kld_loss either after a fixed interval or if a plateau is detected.\n",
    "                if epoch == add_loss_on_epoch + interval_kld or (plateau_count >= plateau_threshold and not kld_introduced):\n",
    "                    beta = original_beta\n",
    "                    kld_introduced = True\n",
    "                    print(\"\\n\\nIntroducing the KLD loss\\n\\n\")\n",
    "                    plateau_count = 0  # Reset plateau count after introducing a loss\n",
    "\n",
    "                # Introduce perceptual_loss either after a fixed interval or if a plateau is detected.\n",
    "                elif epoch == add_loss_on_epoch + interval_perceptual or (plateau_count >= plateau_threshold and not perceptual_introduced):\n",
    "                    theta = original_theta\n",
    "                    perceptual_introduced = True\n",
    "                    print(\"\\n\\nIntroducing the perceptual loss\\n\\n\")\n",
    "                    plateau_count = 0  # Reset plateau count after introducing a loss\n",
    "\n",
    "                # Introduce hist_loss either after a fixed interval or if a plateau is detected.\n",
    "                elif epoch == add_loss_on_epoch + interval_hist or (plateau_count >= plateau_threshold and not hist_introduced):\n",
    "                    lamda = original_lamda\n",
    "                    hist_introduced = True\n",
    "                    print(\"\\n\\nIntroducing the histogram loss\\n\\n\")\n",
    "                    plateau_count = 0  # Reset plateau count after introducing a loss\n",
    "\n",
    "                prev_val_loss = epoch_loss\n",
    "                \n",
    "\n",
    "            \n",
    "            # Visualize generated images during the training.\n",
    "            if phase == 'train':\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    \n",
    "                    # Save original and reconstructed images from the training set.\n",
    "                    save_image(fixed_train_batch[:10], f\"{images_folder}/original_train.png\", nrow=10)\n",
    "                    recon_train, _, _ = model(fixed_train_batch, fixed_train_labels)\n",
    "                    denormalized_recon_train = denormalize(recon_train.clone())\n",
    "                    save_image(recon_train[:10], f\"{images_folder}/recon_train_epoch_{epoch:03}.png\", nrow=10)\n",
    "                    \n",
    "                    # Display images.\n",
    "                    if image_show:\n",
    "                        \n",
    "                        grid_img_train_tensor = make_grid(recon_train[:images_to_show], nrow=nrow)\n",
    "                        grid_img_train_resized = F.interpolate(grid_img_train_tensor.unsqueeze(0), size=(96, 128 * images_to_show), mode='bilinear', align_corners=True).squeeze(0)\n",
    "                        plt.imshow(grid_img_train_resized.permute(1, 2, 0).cpu().numpy())\n",
    "                        plt.title(f\"Training Reconstruction - Epoch {epoch}\")\n",
    "                        plt.axis('off')\n",
    "                        plt.show()\n",
    "\n",
    "            # For the validation phase\n",
    "            elif phase == 'val':\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    \n",
    "                    # Save original and reconstructed images from the validation set.\n",
    "                    save_image(fixed_val_batch[:10], f\"{images_folder}/original_val.png\", nrow=10)\n",
    "                    recon_val, _, _ = model(fixed_val_batch, fixed_val_labels)\n",
    "                    denormalized_recon_val = denormalize(recon_val.clone())\n",
    "                    save_image(recon_val[:10], f\"{images_folder}/recon_val_epoch_{epoch:03}.png\", nrow=10)\n",
    "                    \n",
    "                    # Display images.\n",
    "                    if image_show:\n",
    "                        \n",
    "                        grid_img_val_tensor = make_grid(recon_val[:images_to_show], nrow=nrow)\n",
    "                        grid_img_val_resized = F.interpolate(grid_img_val_tensor.unsqueeze(0), size=(96, 128 * images_to_show), mode='bilinear', align_corners=True).squeeze(0)\n",
    "                        plt.imshow(grid_img_val_resized.permute(1, 2, 0).cpu().numpy())\n",
    "                        plt.title(f\"Validation Reconstruction - Epoch {epoch}\")\n",
    "                        plt.axis('off')\n",
    "                        plt.show()\n",
    "\n",
    "                model.train()\n",
    "\n",
    "            # Update best model if current validation loss is lower.\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = model.state_dict()\n",
    "                no_improve = 0\n",
    "            elif phase == 'val':\n",
    "                no_improve += 1\n",
    "                \n",
    "            \n",
    "        # Check if it's time to analyze the latent space (every 10 epochs).\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            # Re-calculate the correlation matrix and perform analysis\n",
    "            flattened_latents = np.vstack(latent_vectors)\n",
    "            corr_matrix = np.corrcoef(flattened_latents, rowvar=False)\n",
    "\n",
    "            # Plot the correlation matrix\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            plt.imshow(corr_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "            plt.colorbar()\n",
    "            plt.title(f\"Latent Variable Correlation Matrix - Epoch {epoch}\")\n",
    "            plt.show()\n",
    "\n",
    "            # Perform latent space analysis\n",
    "            analyze_latent_space(corr_matrix, flattened_latents)\n",
    "\n",
    "            # Reset latent_vectors for the next set of epochs\n",
    "            latent_vectors = []\n",
    "\n",
    "                \n",
    "        # Calculate the elapsed time and estimate the remaining time at the end of the epoch.\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        start_times.append(elapsed_time)\n",
    "        avg_time_per_epoch = sum(start_times) / len(start_times)\n",
    "        estimated_time_left = avg_time_per_epoch * (num_epochs - epoch - 1)\n",
    "        print(f\"Estimated time left: {estimated_time_left // 3600:.0f}h {(estimated_time_left % 3600) // 60:.0f}m {estimated_time_left % 60:.0f}s\")\n",
    "        \n",
    "        print()\n",
    "\n",
    "        # Implement early stopping.\n",
    "        if no_improve >= patience:\n",
    "            print(\"Early stopping due to no improvement.\")\n",
    "            break\n",
    "\n",
    "    print(\"Best val loss: {:4f}\\n\".format(best_loss))\n",
    "    \n",
    "    # Save best model weights and losses.\n",
    "    torch.save(best_model_wts, os.path.join(images_folder, \"best_conv_model_weights.pth\"))\n",
    "    with open(os.path.join(images_folder, \"conv_losses.pkl\"), \"wb\") as f:\n",
    "        pickle.dump({'train': train_losses, 'val': val_losses}, f)\n",
    "        \n",
    "    # plot_losses_and_save(images_folder, hyperparameters['beta'], hyperparameters['latent_dim'], hyperparameters['loss_type'])\n",
    "    # plot_losses_and_save(images_folder, hyperparameters['beta'], hyperparameters['latent_dim'], hyperparameters['loss_type'], scaled=True)\n",
    "\n",
    "    # Load best model weights.\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model, {\n",
    "        'mse_loss': epoch_mse_loss, \n",
    "        'perceptual_loss': epoch_perceptual_loss, \n",
    "        'hist_loss': epoch_hist_loss\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d427a8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model with combined MSE, KLD, perceptual and histogram loss.\n",
    "\n",
    "# Define hyperparameters.\n",
    "hyperparameters = {\n",
    "    \"num_epochs\": 501,           # Total number of epochs for training\n",
    "    \"patience\": 150,             # Number of epochs with no improvement after which training will be stopped (early stopping)\n",
    "    \"save_interval\": 1,          # Interval at which the model checkpoints are saved\n",
    "    \"learning_rate\": 0.0001,     # Learning rate for the optimizer\n",
    "    \"weight_decay\": 1e-2,        # L2 regularization coefficient, helps in preventing overfitting\n",
    "    \"latent_dim\": 512,           # Dimensionality of the latent space in the CVAE\n",
    "    \"condition_dim\": 2,          # Dimensionality of the condition vector (e.g., 2 for two scores in our case)\n",
    "    \"loss_type\": 'combined',     # Type of the loss function to use ('mse', 'ms-ssim', or 'combined')\n",
    "    \"plateau_threshold\": 10,     # Number of epochs to wait before considering it a plateau (for learning rate adjustment or adding new losses)\n",
    "    \"alpha\": 0.01,               # Weight for the MSE term in the loss when using 'combined' loss_type\n",
    "    \"beta\": 0.1,                 # Weight for the KL-divergence term in the loss\n",
    "    \"theta\": 1000,               # Weight for the perceptual loss term\n",
    "    \"lamda\": 100,                # Weight for the histogram loss term\n",
    "    \"add_loss_on_epoch\": 30,     # Epoch number after which additional loss terms start getting added\n",
    "    \"interval_perceptual\": 0,    # Interval after 'add_loss_on_epoch' to introduce perceptual loss\n",
    "    \"interval_kld\": 20,          # Interval after 'add_loss_on_epoch' to introduce KLD loss\n",
    "    \"interval_hist\": 100,        # Interval after 'add_loss_on_epoch' to introduce histogram loss\n",
    "    \"images_to_show\": 5,         # Number of images to display during training for visual inspection\n",
    "    \"nrow\": 5,                   # Number of rows when showing the images using `make_grid`\n",
    "    \"image_show\": True,          # Whether to show images during training\n",
    "    \"debug\": False               # If true, print out additional debug information\n",
    "}\n",
    "\n",
    "# Create a model instance and initialize optimizer and scheduler.\n",
    "cvae = CVAE_ResNet18_DualEmbedding(conditional_dim=2, latent_dim=hyperparameters['latent_dim'], debug=False).to(device)\n",
    "optimizer = torch.optim.Adam(cvae.parameters(), lr=hyperparameters['learning_rate'], weight_decay=hyperparameters['weight_decay'])\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "\n",
    "# Train the model instance and vizualize losses.\n",
    "trained_model = train_and_visualize_losses(cvae, train_loader, val_loader, optimizer, scheduler, hyperparameters) # Hyperparameters can be changed above before the function call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff84f9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model with combined MSE, KLD, perceptual and histogram loss.\n",
    "\n",
    "# Define hyperparameters.\n",
    "hyperparameters = {\n",
    "    \"num_epochs\": 301,           # Total number of epochs for training\n",
    "    \"patience\": 150,             # Number of epochs with no improvement after which training will be stopped (early stopping)\n",
    "    \"save_interval\": 1,          # Interval at which the model checkpoints are saved\n",
    "    \"learning_rate\": 0.0001,     # Learning rate for the optimizer\n",
    "    \"weight_decay\": 1e-2,        # L2 regularization coefficient, helps in preventing overfitting\n",
    "    \"latent_dim\": 512,           # Dimensionality of the latent space in the CVAE\n",
    "    \"condition_dim\": 2,          # Dimensionality of the condition vector (e.g., 2 for two scores in our case)\n",
    "    \"loss_type\": 'combined',     # Type of the loss function to use ('mse', 'ms-ssim', or 'combined')\n",
    "    \"plateau_threshold\": 10,     # Number of epochs to wait before considering it a plateau (for learning rate adjustment or adding new losses)\n",
    "    \"alpha\": 0.01,               # Weight for the MSE term in the loss when using 'combined' loss_type\n",
    "    \"beta\": 0.1,                 # Weight for the KL-divergence term in the loss\n",
    "    \"theta\": 1000,               # Weight for the perceptual loss term\n",
    "    \"lamda\": 100,                # Weight for the histogram loss term\n",
    "    \"add_loss_on_epoch\": 50,     # Epoch number after which additional loss terms start getting added\n",
    "    \"interval_perceptual\": 0,    # Interval after 'add_loss_on_epoch' to introduce perceptual loss\n",
    "    \"interval_kld\": 20,          # Interval after 'add_loss_on_epoch' to introduce KLD loss\n",
    "    \"interval_hist\": 100,        # Interval after 'add_loss_on_epoch' to introduce histogram loss\n",
    "    \"images_to_show\": 5,         # Number of images to display during training for visual inspection\n",
    "    \"nrow\": 5,                   # Number of rows when showing the images using `make_grid`\n",
    "    \"image_show\": True,          # Whether to show images during training\n",
    "    \"debug\": False               # If true, print out additional debug information\n",
    "}\n",
    "\n",
    "# Create a model instance and initialize optimizer and scheduler.\n",
    "cvae = CVAE_ResNet18_DualEmbedding(conditional_dim=2, latent_dim=hyperparameters['latent_dim'], debug=False).to(device)\n",
    "optimizer = torch.optim.Adam(cvae.parameters(), lr=hyperparameters['learning_rate'], weight_decay=hyperparameters['weight_decay'])\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "\n",
    "# Train the model instance and vizualize losses.\n",
    "trained_model = train_and_visualize_losses(cvae, train_loader, val_loader, optimizer, scheduler, hyperparameters) # Hyperparameters can be changed above before the function call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee02d6c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model with combined MSE, KLD, perceptual and histogram loss.\n",
    "\n",
    "# Define hyperparameters.\n",
    "hyperparameters = {\n",
    "    \"num_epochs\": 301,           # Total number of epochs for training\n",
    "    \"patience\": 150,             # Number of epochs with no improvement after which training will be stopped (early stopping)\n",
    "    \"save_interval\": 1,          # Interval at which the model checkpoints are saved\n",
    "    \"learning_rate\": 0.0001,     # Learning rate for the optimizer\n",
    "    \"weight_decay\": 1e-2,        # L2 regularization coefficient, helps in preventing overfitting\n",
    "    \"latent_dim\": 512,           # Dimensionality of the latent space in the CVAE\n",
    "    \"condition_dim\": 2,          # Dimensionality of the condition vector (e.g., 2 for two scores in our case)\n",
    "    \"loss_type\": 'ms-ssim',      # Type of the loss function to use ('mse', 'ms-ssim', or 'combined')\n",
    "    \"plateau_threshold\": 10,     # Number of epochs to wait before considering it a plateau (for learning rate adjustment or adding new losses)\n",
    "    \"alpha\": 0.01,               # Weight for the MSE term in the loss when using 'combined' loss_type\n",
    "    \"beta\": 1,                   # Weight for the KL-divergence term in the loss\n",
    "    \"theta\": 0,                  # Weight for the perceptual loss term\n",
    "    \"lamda\": 0,                  # Weight for the histogram loss term\n",
    "    \"add_loss_on_epoch\": 50,     # Epoch number after which additional loss terms start getting added\n",
    "    \"interval_perceptual\": 300,  # Interval after 'add_loss_on_epoch' to introduce perceptual loss\n",
    "    \"interval_kld\": 0,           # Interval after 'add_loss_on_epoch' to introduce KLD loss\n",
    "    \"interval_hist\": 300,        # Interval after 'add_loss_on_epoch' to introduce histogram loss\n",
    "    \"images_to_show\": 5,         # Number of images to display during training for visual inspection\n",
    "    \"nrow\": 5,                   # Number of rows when showing the images using `make_grid`\n",
    "    \"image_show\": True,          # Whether to show images during training\n",
    "    \"debug\": False               # If true, print out additional debug information\n",
    "}\n",
    "\n",
    "# Create a model instance and initialize optimizer and scheduler.\n",
    "cvae = CVAE_ResNet18_DualEmbedding(conditional_dim=2, latent_dim=hyperparameters['latent_dim'], debug=False).to(device)\n",
    "optimizer = torch.optim.Adam(cvae.parameters(), lr=hyperparameters['learning_rate'], weight_decay=hyperparameters['weight_decay'])\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "\n",
    "# Train the model instance and vizualize losses.\n",
    "trained_model = train_and_visualize_losses(cvae, train_loader, val_loader, optimizer, scheduler, hyperparameters) # Hyperparameters can be changed above before the function call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81159332",
   "metadata": {},
   "source": [
    "## Run for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22bc509",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check if the folder contains the .pth and .pkl files (i.e.trained model, hyperparameters and losses)\n",
    "def check_folders_for_files(base_folder):\n",
    "    required_files = [\"best_conv_model_weights.pth\", \"conv_losses.pkl\", \"hyperparameters.txt\"]\n",
    "    valid_folders = []\n",
    "\n",
    "    for folder in os.listdir(base_folder):\n",
    "        if folder.startswith(\"training_images_2023\"):\n",
    "            all_files_present = all(os.path.exists(os.path.join(base_folder, folder, file)) for file in required_files)\n",
    "            if all_files_present:\n",
    "                valid_folders.append(folder)\n",
    "    return sorted(valid_folders)\n",
    "\n",
    "# Rename empty folders without the .pth and .pkl files (i.e.trained model, hyperparameters and losses) \n",
    "def rename_folders(root_folder, valid_folders):\n",
    "    # Identify folders to be renamed\n",
    "    folders_to_rename = sorted([folder for folder in os.listdir(root_folder) if folder.startswith(\"training_images_2023\") and folder not in valid_folders])\n",
    "    \n",
    "    # If there are no folders to rename, exit early\n",
    "    if not folders_to_rename:\n",
    "        print(\"No folders need renaming.\")\n",
    "        return\n",
    "    \n",
    "    # List folders to be renamed\n",
    "    print(\"Folders to be renamed:\")\n",
    "    for folder in folders_to_rename:\n",
    "        print(folder)\n",
    "\n",
    "    # Ask for confirmation\n",
    "    confirm = input(\"\\nDo you want to rename these folders? (y/n): \")\n",
    "    \n",
    "    if confirm.lower() == 'y':\n",
    "        for folder in folders_to_rename:\n",
    "            old_folder_path = os.path.join(root_folder, folder)\n",
    "            new_folder_path = os.path.join(root_folder, \"empty_\" + folder)\n",
    "            os.rename(old_folder_path, new_folder_path)\n",
    "            print(f\"Renamed: {old_folder_path} -> {new_folder_path}\")\n",
    "    else:\n",
    "        print(\"No folders were renamed.\")\n",
    "\n",
    "def get_folders_in_results_directory(base_path=\"results/\"):\n",
    "    folders = [f for f in os.listdir(base_path) if f.startswith(\"training_images_2023\") and os.path.isdir(os.path.join(base_path, f))]\n",
    "    return sorted(folders)  # Sort the folders by name which effectively sorts them by date\n",
    "\n",
    "def extract_hyperparameters(folder):\n",
    "    \"\"\"Extract the required hyperparameters from the hyperparameters.txt file.\"\"\"\n",
    "    hyperparameters = {}\n",
    "    with open(os.path.join(folder, \"hyperparameters.txt\"), \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            if \":\" in line:  # Check if the line contains a colon\n",
    "                parts = line.strip().split(\": \")\n",
    "                key = parts[0]\n",
    "                value = parts[1]\n",
    "                hyperparameters[key] = value\n",
    "    return hyperparameters\n",
    "\n",
    "def load_model_weights(model, folder_path):\n",
    "    # Load the model weights from the specified folder\n",
    "    weights_path = os.path.join(folder_path, \"best_conv_model_weights.pth\")\n",
    "    if not os.path.exists(weights_path):\n",
    "        raise FileNotFoundError(f\"No weights file found at {weights_path}\")\n",
    "    model.load_state_dict(torch.load(weights_path, map_location=device))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3777f973",
   "metadata": {},
   "source": [
    "## Load weights and generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cd992d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check if the folder contains the .pth and .pkl files (i.e.trained model, hyperparameters and losses)\n",
    "def check_folders_for_files(base_folder):\n",
    "    required_files = [\"best_conv_model_weights.pth\", \"conv_losses.pkl\", \"hyperparameters.txt\"]\n",
    "    valid_folders = []\n",
    "\n",
    "    for folder in os.listdir(base_folder):\n",
    "        if folder.startswith(\"training_images_2023\"):\n",
    "            all_files_present = all(os.path.exists(os.path.join(base_folder, folder, file)) for file in required_files)\n",
    "            if all_files_present:\n",
    "                valid_folders.append(folder)\n",
    "    return sorted(valid_folders)\n",
    "\n",
    "# Rename empty folders without the .pth and .pkl files (i.e.trained model, hyperparameters and losses) \n",
    "def rename_folders(root_folder, valid_folders):\n",
    "    # Identify folders to be renamed\n",
    "    folders_to_rename = sorted([folder for folder in os.listdir(root_folder) if folder.startswith(\"training_images_2023\") and folder not in valid_folders])\n",
    "    \n",
    "    # If there are no folders to rename, exit early\n",
    "    if not folders_to_rename:\n",
    "        print(\"No folders need renaming.\")\n",
    "        return\n",
    "    \n",
    "    # List folders to be renamed\n",
    "    print(\"Folders to be renamed:\")\n",
    "    for folder in folders_to_rename:\n",
    "        print(folder)\n",
    "\n",
    "    # Ask for confirmation\n",
    "    confirm = input(\"\\nDo you want to rename these folders? (y/n): \")\n",
    "    \n",
    "    if confirm.lower() == 'y':\n",
    "        for folder in folders_to_rename:\n",
    "            old_folder_path = os.path.join(root_folder, folder)\n",
    "            new_folder_path = os.path.join(root_folder, \"empty_\" + folder)\n",
    "            os.rename(old_folder_path, new_folder_path)\n",
    "            print(f\"Renamed: {old_folder_path} -> {new_folder_path}\")\n",
    "    else:\n",
    "        print(\"No folders were renamed.\")\n",
    "  \n",
    "            \n",
    "# Check folders and print results\n",
    "result_folders = check_folders_for_files(\"results\")\n",
    "print(\"Folders containing the required files:\")\n",
    "for folder in result_folders:\n",
    "    print(folder)\n",
    "    \n",
    "print()\n",
    "\n",
    "# Rename folders \n",
    "rename_folders(\"results\", result_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459df8b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_and_save_images_and_embeddings(model, dataloader, num_samples=10, device=\"cuda\"):\n",
    "    # 1. Get some images and their associated condition vectors from the dataloader\n",
    "    images, c_list = next(iter(dataloader))\n",
    "    \n",
    "    # Debug messages to check the type and shape of images and c\n",
    "    print(f\"Type of images: {type(images)}, Shape of images: {images.shape}\")\n",
    "    print(f\"Type of c_list: {type(c_list)}\")\n",
    "    \n",
    "    # Stack the scores for Moira and Ferdinando along the second dimension\n",
    "    c = torch.stack(c_list, dim=1).to(device)\n",
    "    print(f\"Combined condition tensor shape: {c.shape}\")\n",
    "    \n",
    "    images = images.to(device)[:num_samples]\n",
    "    c = c[:num_samples]\n",
    "\n",
    "    # 2. Pass the images and their condition vectors through the model to get the embeddings       \n",
    "    with torch.no_grad():\n",
    "        mu, logvar = model.encode(images, c)\n",
    "        z = model.reparameterize(mu, logvar)\n",
    "        embeddings = torch.cat([z, c], dim=1)  # concatenate conditions to embeddings before saving\n",
    "\n",
    "\n",
    "    # 3. Save the images and embeddings to the specified paths\n",
    "    torch.save(images.cpu(), 'ready/images.pth')\n",
    "    torch.save(embeddings.cpu(), 'ready/embeddings.pth')\n",
    "    \n",
    "    # 4. Save the images to a visual format for verification\n",
    "    save_image(images.cpu(), 'ready/samples.png', nrow=int(num_samples**1), padding=2, normalize=True)\n",
    "    \n",
    "    # 5. Define the source and destination paths for the model\n",
    "    src_path = os.path.join(folder_path, \"best_conv_model_weights.pth\")\n",
    "    dest_path = 'ready/best_conv_model_weights.pth'\n",
    "    \n",
    "    # 6. Use shutil to copy the file\n",
    "    import shutil\n",
    "    shutil.copy(src_path, dest_path)\n",
    "    print(f\"Images and embeddings have been saved. Model weights copied to: {dest_path}.\")\n",
    "\n",
    "\n",
    "# Assuming the model class and structure is defined, instantiate the model\n",
    "model = CVAE_ResNet18_DualEmbedding()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "# Load the model weights\n",
    "base_folder = \"results\"\n",
    "valid_folders = check_folders_for_files(base_folder)\n",
    "\n",
    "if valid_folders:\n",
    "    # Display valid folders and ask user to choose\n",
    "    for idx, folder_name in enumerate(valid_folders, 1):\n",
    "        print(f\"{idx}. {folder_name}\")\n",
    "    \n",
    "    chosen_idx = int(input(\"\\nChoose a folder: \")) - 1\n",
    "    if chosen_idx < 0 or chosen_idx >= len(valid_folders):\n",
    "        raise ValueError(\"Invalid choice. Please choose a valid number.\")\n",
    "    \n",
    "    chosen_folder = valid_folders[chosen_idx]\n",
    "    folder_path = os.path.join(base_folder, chosen_folder)\n",
    "    model = load_model_weights(model, folder_path)\n",
    "    \n",
    "    print(f\"Model weights loaded from: {chosen_folder}\")\n",
    "    \n",
    "    # Generate and save images and embeddings with the chosen model\n",
    "    generate_and_save_images_and_embeddings(model, train_loader, device=device)\n",
    "else:\n",
    "    raise ValueError(\"No valid folders found with trained models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe4fa5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the saved images\n",
    "saved_images = torch.load('ready/images.pth')\n",
    "resized_images = F.interpolate(saved_images, size=(96, 128))  # Resize to 128x96\n",
    "\n",
    "# Visualize the first few images\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 5))\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(resized_images[i].permute(1, 2, 0).cpu().numpy())\n",
    "    ax.set_title(f\"Original Image {i+1}\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76750b3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_losses_and_save(images_folder, hyperparameters['beta'], hyperparameters['latent_dim'], hyperparameters['loss_type'])\n",
    "plot_losses_and_save(images_folder, hyperparameters['beta'], hyperparameters['latent_dim'], hyperparameters['loss_type'], scaled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16c308d",
   "metadata": {},
   "source": [
    "## DIsplay reconstructed images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9406b8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_folders_in_results_directory():\n",
    "    return sorted([folder for folder in os.listdir('results/') if os.path.isdir(os.path.join('results/', folder)) and folder.startswith('training_images')])\n",
    "\n",
    "def display_folder_list(folders):\n",
    "    print(\"Available folders:\")\n",
    "    for idx, folder in enumerate(folders):\n",
    "        print(f\"{idx + 1}. {folder}\")\n",
    "    print(\"\\nEnter the number of the folder to process:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a967df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def display_images(folder_path, img_type='train'):\n",
    "    if img_type == 'train':\n",
    "        original_img_path = os.path.join(folder_path, \"original_train.png\")\n",
    "    else:  # 'val'\n",
    "        original_img_path = os.path.join(folder_path, \"original_val.png\")\n",
    "        \n",
    "    original_img = Image.open(original_img_path)\n",
    "    original_img = original_img.resize((128*10, 96))  # Resize to 128x96\n",
    "    \n",
    "    image_paths = sorted(glob.glob(f\"{folder_path}/recon_{img_type}_epoch_*.png\"))\n",
    "    \n",
    "    for image_path in image_paths:\n",
    "        recon_img = Image.open(image_path)\n",
    "        recon_img = recon_img.resize((128*10, 96))  # Resize to 128x96\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        plt.figure(figsize=(15, 3))  # figure size for vertical display\n",
    "        \n",
    "        # Display original image\n",
    "        plt.subplot(2, 1, 1)  # 2x1 grid\n",
    "        plt.imshow(original_img)\n",
    "        plt.title(\"Original\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Display reconstructed image\n",
    "        plt.subplot(2, 1, 2)  # 2x1 grid\n",
    "        plt.imshow(recon_img)\n",
    "        plt.title(os.path.basename(image_path))\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        time.sleep(0.01)\n",
    "      \n",
    "\n",
    "# List folders\n",
    "folders = get_folders_in_results_directory()\n",
    "display_folder_list(folders)\n",
    "\n",
    "# Get user input for folder\n",
    "selected_folder_num = input().strip()\n",
    "\n",
    "# Default to latest folder if no input is given or invalid input\n",
    "if not selected_folder_num or not selected_folder_num.isdigit() or int(selected_folder_num) > len(folders):\n",
    "    selected_folder = folders[-1]  # Latest folder\n",
    "else:\n",
    "    selected_folder = folders[int(selected_folder_num) - 1]\n",
    "\n",
    "# Get user input for image type\n",
    "img_type_choice = input(\"\\nChoose image type:\\n1. Training\\n2. Validation\\nEnter choice number (default is Training): \").strip()\n",
    "img_type = 'train' if img_type_choice != '2' else 'val'\n",
    "\n",
    "clear_output(wait=True)\n",
    "print(f\"Displaying images from: {selected_folder}\")\n",
    "display_images(os.path.join('results/', selected_folder), img_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f6de00",
   "metadata": {},
   "source": [
    "## Save reconstructed images as a Gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29b2050",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def save_images_to_gif(folder_path, gif_filename, img_type='train'):\n",
    "    # Depending on the choice of image type, select the original image.\n",
    "    if img_type == 'train':\n",
    "        original_img_path = os.path.join(folder_path, \"original_train.png\")\n",
    "    else:  # 'val'\n",
    "        original_img_path = os.path.join(folder_path, \"original_val.png\")\n",
    "        \n",
    "    original_img = Image.open(original_img_path)\n",
    "    original_img = original_img.resize((128*10, 96))\n",
    "    \n",
    "    # Filter the image paths based on the chosen type.\n",
    "    all_image_paths = sorted(glob.glob(f\"{folder_path}/recon_{img_type}_epoch_*.png\"))\n",
    "    \n",
    "    # Select every 10th image and always include the last one\n",
    "    image_paths = all_image_paths[::10] + [all_image_paths[-1]] if len(all_image_paths) > 1 else all_image_paths\n",
    "    \n",
    "    gif_images = []\n",
    "    \n",
    "    # Wrap the loop with tqdm for a progress bar.\n",
    "    for image_path in tqdm(image_paths, desc=\"Generating GIF Image\", ncols=100):\n",
    "        recon_img = Image.open(image_path)\n",
    "        recon_img = recon_img.resize((128*10, 96))\n",
    "        \n",
    "        fig, axarr = plt.subplots(2, 1, figsize=(12, 3))\n",
    "        \n",
    "        # Display original image.\n",
    "        axarr[0].imshow(original_img)\n",
    "        axarr[0].set_title(\"Original\")\n",
    "        axarr[0].axis('off')\n",
    "        \n",
    "        # Display reconstructed image.\n",
    "        axarr[1].imshow(recon_img)\n",
    "        axarr[1].set_title(os.path.basename(image_path))\n",
    "        axarr[1].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the current figure to a BytesIO stream.\n",
    "        buf = io.BytesIO()\n",
    "        plt.savefig(buf, format='png', dpi=150)\n",
    "        buf.seek(0)\n",
    "        gif_images.append(Image.open(buf))\n",
    "        plt.close()\n",
    "        \n",
    "    # Save gif_images to a GIF using Pillow with a fixed duration of 300ms per frame.\n",
    "    gif_images[0].save(gif_filename, save_all=True, append_images=gif_images[1:], loop=0, duration=300)\n",
    "    \n",
    "# List folders.\n",
    "folders = get_folders_in_results_directory()\n",
    "display_folder_list(folders)\n",
    "\n",
    "# Get user input for folder.\n",
    "selected_folder_num = input().strip()\n",
    "\n",
    "# Default to latest folder if no input is given or invalid input.\n",
    "if not selected_folder_num or not selected_folder_num.isdigit() or int(selected_folder_num) > len(folders):\n",
    "    selected_folder = folders[-1]\n",
    "else:\n",
    "    selected_folder = folders[int(selected_folder_num) - 1]\n",
    "\n",
    "clear_output(wait=True)\n",
    "print(f\"Generating GIFs from: {selected_folder}\")\n",
    "\n",
    "for img_type in ['train', 'val']:\n",
    "    # Save inside 'results/gif/' with the appropriate suffix based on the image type.\n",
    "    gif_filename = os.path.join('results', 'gif', f\"{os.path.basename(selected_folder)}_{img_type}_reconstruction.gif\")\n",
    "    os.makedirs('results/gif', exist_ok=True)\n",
    "    save_images_to_gif(os.path.join('results/', selected_folder), gif_filename, img_type)\n",
    "    print(f\"{img_type} gif saved as: {gif_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9f7eb6",
   "metadata": {},
   "source": [
    "## Load the trained model and generate images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a08693f",
   "metadata": {},
   "source": [
    "**Ensemble Strategy:** Generate images for each score above the threshold and then average the generated images.\n",
    "\n",
    "For threshold 7, generating images for scores 7, 8, and 9, then average these images to get the final image.\n",
    "\n",
    "Pros: Easy to implement, no retraining required.\n",
    "Cons: May produce blurry images due to averaging.\n",
    "__________\n",
    "\n",
    "- for a threshold of 0, we generate and average images for scores 0 through 9.\n",
    "\n",
    "- for a threshold of 7, we generate and average images for scores 7 through 9.\n",
    "\n",
    "- for a threshold of 9, we would generate an image only for score 9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86993247",
   "metadata": {},
   "source": [
    "**Probability Strategy:** Instead of setting a discrete score in the condition vector, using a probability distribution.\n",
    "\n",
    "For threshold 7, set probabilities for scores 7, 8, and 9 to be higher than the rest.\n",
    "\n",
    "Generating the image using this modified condition vector.\n",
    "Pros: More flexible representation.\n",
    "Cons: Might produce images that don't strongly correlate to any specific score.\n",
    "\n",
    "______\n",
    "- For scores above the threshold, assigning higher probabilities.\n",
    "- For scores below the threshold, assigning lower probabilities.\n",
    "- Using this probability distribution to sample a score.\n",
    "- Generating the image using the sampled score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d53762",
   "metadata": {},
   "source": [
    "**Iterative Refinement:** Starting with the lowest acceptable score and iteratively refine the image.\n",
    "\n",
    "Generating an image with score 7. Using this image as an input and condition with score 8, and so on.\n",
    "\n",
    "Pros: Sequential enhancement of images.\n",
    "Cons: Multiple forward passes, works a bit slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb53726",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def get_folders_in_results_directory(base_path=\"results/\"):\n",
    "    folders = [f for f in os.listdir(base_path) if f.startswith(\"training_images_20231015\") and os.path.isdir(os.path.join(base_path, f))]\n",
    "    return sorted(folders)  # Sort the folders by name which effectively sorts them by date\n",
    "\n",
    "def extract_hyperparameters(folder):\n",
    "    \"\"\"Extract the required hyperparameters from the hyperparameters.txt file.\"\"\"\n",
    "    hyperparameters = {}\n",
    "    with open(os.path.join(folder, \"hyperparameters.txt\"), \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            if \":\" in line:  # Check if the line contains a colon\n",
    "                parts = line.strip().split(\": \")\n",
    "                key = parts[0]\n",
    "                value = parts[1]\n",
    "                hyperparameters[key] = value\n",
    "    return hyperparameters\n",
    "\n",
    "def load_model_weights(model, folder_path):\n",
    "    # Load the model weights from the specified folder\n",
    "    weights_path = os.path.join(folder_path, \"best_conv_model_weights.pth\")\n",
    "    if not os.path.exists(weights_path):\n",
    "        raise FileNotFoundError(f\"No weights file found at {weights_path}\")\n",
    "    model.load_state_dict(torch.load(weights_path))\n",
    "    return model\n",
    "\n",
    "\n",
    "def tensor_to_image(tensor):\n",
    "    tensor = tensor.detach().clamp(0, 1)\n",
    "    tensor = tensor.permute(1, 2, 0)\n",
    "    return tensor.cpu().numpy()\n",
    "\n",
    "\n",
    "def generate_images_for_strategy(model, strategy, hyperparameters):\n",
    "    if strategy == \"ensemble\":\n",
    "        return generate_images_for_thresholds(model, int(hyperparameters[\"condition_dim\"]), hyperparameters)\n",
    "    elif strategy == \"probability\":\n",
    "        return generate_images_with_probability(model, int(hyperparameters[\"condition_dim\"]), hyperparameters)\n",
    "    elif strategy == \"refinement\":\n",
    "        start_score = 7\n",
    "        end_score = 9\n",
    "        return iterative_refinement(model, int(hyperparameters[\"condition_dim\"]), start_score, end_score, hyperparameters)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid strategy\")\n",
    "\n",
    "        \n",
    "def generate_and_display_images_for_each_folder(strategy=\"ensemble\", display=True, base_path=\"results/\"):\n",
    "    folders = get_folders_in_results_directory()\n",
    "\n",
    "    # Iterate over each folder and generate images\n",
    "    for folder in folders:\n",
    "        full_folder_path = os.path.join(base_path, folder)\n",
    "\n",
    "        # Extract hyperparameters\n",
    "        hyperparameters = extract_hyperparameters(full_folder_path)\n",
    "\n",
    "        # Create model instance based on the stored hyperparameters\n",
    "        # Use default values if certain hyperparameters aren't found\n",
    "        condition_dim = int(hyperparameters.get('condition_dim', 2))\n",
    "        latent_dim = int(hyperparameters.get('latent_dim', 512))\n",
    "\n",
    "        try:\n",
    "            model_instance = CVAE_ResNet18_DualEmbedding_(conditional_dim=condition_dim,\n",
    "                                                          latent_dim=latent_dim,\n",
    "                                                          debug=False).to(device)\n",
    "            trained_model = load_model_weights(model_instance, full_folder_path)\n",
    "        except:\n",
    "            model_instance = CVAE_ResNet18_DualEmbedding(conditional_dim=condition_dim,\n",
    "                                                         latent_dim=latent_dim,\n",
    "                                                         debug=False).to(device)\n",
    "            trained_model = load_model_weights(model_instance, full_folder_path)\n",
    "\n",
    "        # Generate embeddings for 10 images\n",
    "        z = torch.randn(10, latent_dim).to(device)\n",
    "        \n",
    "        # Assuming a constant condition for simplicity\n",
    "        c = torch.zeros(10, condition_dim).to(device)\n",
    "\n",
    "        # Decode to get images\n",
    "        with torch.no_grad():\n",
    "            generated_images_tensor = trained_model.decode(z, c)\n",
    "        generated_images = [tensor_to_image(img) for img in generated_images_tensor]\n",
    "\n",
    "        # Save the images and embeddings\n",
    "        torch.save(generated_images_tensor.cpu(), os.path.join(full_folder_path, 'generated_images.pth'))\n",
    "        torch.save(z.cpu(), os.path.join(full_folder_path, 'embeddings.pth'))\n",
    "\n",
    "        # Display and save the images as well\n",
    "        save_path = os.path.join(full_folder_path, f\"generated_images/{strategy}_images\", f\"{strategy}_{folder}.png\")\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "        fig, axes = plt.subplots(1, len(generated_images), figsize=(20, 2))\n",
    "        if not isinstance(axes, np.ndarray):  # If axes is not an array (i.e., single axes object)\n",
    "            axes = [axes]\n",
    "\n",
    "        for ax, img in zip(axes, generated_images):\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "        fig.suptitle(folder, y=1.05)\n",
    "        plt.savefig(save_path, bbox_inches='tight')\n",
    "\n",
    "        if display:\n",
    "            plt.show()\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "        \n",
    "# Folders we want to visualize images for\n",
    "result_folders = get_folders_in_results_directory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35feb6ec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def display_training_and_validation_images(result_folders):\n",
    "    # Figure for training images\n",
    "    plt.figure(figsize=(20, 4 * len(result_folders)))\n",
    "    \n",
    "    # Iterate over the folders for training images\n",
    "    for idx, folder in enumerate(result_folders):\n",
    "        full_folder_path = os.path.join(\"results\", folder)\n",
    "    \n",
    "        # Extract hyperparameters from the file.\n",
    "        hp = extract_hyperparameters(full_folder_path)\n",
    "    \n",
    "        # Get list of training reconstruction images\n",
    "        train_recons = [img for img in os.listdir(full_folder_path) if img.startswith(\"recon_train_epoch_\")]\n",
    "    \n",
    "        # Get the image with the maximum epoch index for training reconstructions\n",
    "        max_train_epoch = max([int(re.search(r\"(\\d+).png\", img).group(1)) for img in train_recons])\n",
    "        max_train_img_name = f\"recon_train_epoch_{max_train_epoch}.png\"\n",
    "        max_train_img = os.path.join(full_folder_path, max_train_img_name)\n",
    "    \n",
    "        # Display the training images with hyperparameters overlayed\n",
    "        plt.subplot(len(result_folders), 1, idx+1)\n",
    "        img = plt.imread(max_train_img)\n",
    "        plt.imshow(img)\n",
    "        title_str = (f\"Folder: {folder}\\n\" +\n",
    "                     f\"Epoch Index: {max_train_epoch} | File: {max_train_img_name}\\n\" +\n",
    "                     f\"latent_dim: {hp['latent_dim']}, \" +\n",
    "                     f\"loss_type: {hp['loss_type']}, \" +\n",
    "                     f\"learning_rate: {hp['learning_rate']}, \" +\n",
    "                     f\"weight_decay: {hp['weight_decay']}, \" +\n",
    "                     f\"alpha: {hp['alpha']}, \" +\n",
    "                     f\"beta: {hp['beta']}, \" +\n",
    "                     f\"theta: {hp['theta']}, \" +\n",
    "                     f\"lamda: {hp['lamda']}\")\n",
    "        plt.title(title_str, loc='left')\n",
    "        plt.axis('off')\n",
    "\n",
    "    # Save the combined training images as a PNG\n",
    "    train_results_path = os.path.join(\"results\", \"train_results.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(train_results_path)\n",
    "    plt.show()\n",
    "\n",
    "    # Figure for validation images\n",
    "    plt.figure(figsize=(20, 4 * len(result_folders)))\n",
    "    \n",
    "    # Iterate over the folders for validation images\n",
    "    for idx, folder in enumerate(result_folders):\n",
    "        full_folder_path = os.path.join(\"results\", folder)\n",
    "    \n",
    "        # Extract hyperparameters from the file.\n",
    "        hp = extract_hyperparameters(full_folder_path)\n",
    "    \n",
    "        # Get list of validation reconstruction images\n",
    "        val_recons = [img for img in os.listdir(full_folder_path) if img.startswith(\"recon_val_epoch_\")]\n",
    "    \n",
    "        # Get the image with the maximum epoch index for validation reconstructions\n",
    "        max_val_epoch = max([int(re.search(r\"(\\d+).png\", img).group(1)) for img in val_recons])\n",
    "        max_val_img_name = f\"recon_val_epoch_{max_val_epoch}.png\"\n",
    "        max_val_img = os.path.join(full_folder_path, max_val_img_name)\n",
    "    \n",
    "        # Display the validation images with hyperparameters overlayed\n",
    "        plt.subplot(len(result_folders), 1, idx+1)\n",
    "        img = plt.imread(max_val_img)\n",
    "        plt.imshow(img)\n",
    "        title_str = (f\"Folder: {folder}\\n\" +\n",
    "                     f\"Epoch Index: {max_val_epoch} | File: {max_val_img_name}\\n\" +\n",
    "                     f\"latent_dim: {hp['latent_dim']}, \" +\n",
    "                     f\"loss_type: {hp['loss_type']}, \" +\n",
    "                     f\"learning_rate: {hp['learning_rate']}, \" +\n",
    "                     f\"weight_decay: {hp['weight_decay']}, \" +\n",
    "                     f\"alpha: {hp['alpha']}, \" +\n",
    "                     f\"beta: {hp['beta']}, \" +\n",
    "                     f\"theta: {hp['theta']}, \" +\n",
    "                     f\"lamda: {hp['lamda']}\")\n",
    "        plt.title(title_str, loc='left')\n",
    "        plt.axis('off')\n",
    "\n",
    "    # Save the combined validation images as a PNG\n",
    "    val_results_path = os.path.join(\"results\", \"val_results.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(val_results_path)\n",
    "    plt.show()\n",
    "    \n",
    "        \n",
    "# Display training and validation images\n",
    "display_training_and_validation_images(result_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36cb9db",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Function to generate images for direct scores\n",
    "def generate_images_for_all_scores(model, c_dim, hyperparameters):\n",
    "    model.eval()\n",
    "    all_samples = []\n",
    "\n",
    "    for score in range(10):  # for scores from 0 to 9\n",
    "        score_samples = []\n",
    "        for _ in range(10):  # generate 10 images for each score\n",
    "            z = torch.randn(1, int(hyperparameters[\"latent_dim\"])).to(device)\n",
    "\n",
    "            # Setting scores to the current score for both Moira and Ferdinando\n",
    "            condition = torch.zeros(1, c_dim).to(device)\n",
    "            condition[:, 0] = score  # assuming 0 corresponds to Moira's score\n",
    "            condition[:, 1] = score  # assuming 1 corresponds to Ferdinando's score\n",
    "\n",
    "            with torch.no_grad():\n",
    "                sample = model.decode(z, condition)\n",
    "\n",
    "            # Resize the image to 96x128\n",
    "            resized_sample = F.interpolate(sample, size=(96, 128), mode='bilinear', align_corners=False)\n",
    "            score_samples.append(resized_sample.squeeze(0))\n",
    "        all_samples.extend(score_samples)\n",
    "\n",
    "    return torch.stack(all_samples)\n",
    "\n",
    "\n",
    "\n",
    "def display_generated_images_by_score(result_folders):\n",
    "    # Iterate over the folders starting with training_images_2023\n",
    "    for idx, folder in enumerate(result_folders):\n",
    "        full_folder_path = os.path.join(\"results\", folder)\n",
    "    \n",
    "        # Extract hyperparameters from the file.\n",
    "        hp = extract_hyperparameters(full_folder_path)\n",
    "    \n",
    "        # First attempt to load with cvae_ model structure\n",
    "        model = CVAE_ResNet18_DualEmbedding(conditional_dim=int(hp['condition_dim']), \n",
    "                                             latent_dim=int(hp['latent_dim']), debug=False).to(device)\n",
    "    \n",
    "        # Load the model weights\n",
    "        try:\n",
    "            model = load_model_weights(model, full_folder_path)\n",
    "        except:\n",
    "            # If there's an error, try with the cvae model structure\n",
    "            model = CVAE_ResNet18_DualEmbedding(conditional_dim=int(hp['condition_dim']), \n",
    "                                                latent_dim=int(hp['latent_dim']), debug=False).to(device)\n",
    "            try:\n",
    "                model = load_model_weights(model, full_folder_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in folder {folder}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Generate and visualize images for each threshold\n",
    "        generated_images_tensor = generate_images_for_thresholds(model, int(hp[\"condition_dim\"]), hp)\n",
    "        generated_images = [tensor_to_image(img.cpu()) for tensor in generated_images_tensor for img in tensor]\n",
    "\n",
    "        # Plotting the generated images\n",
    "        fig, axes = plt.subplots(10, 10, figsize=(20, 20))\n",
    "    \n",
    "        # Title\n",
    "        max_val_epoch = hp.get(\"num_epochs\", \"N/A\")\n",
    "        title_str = (f\"Folder: {folder}\\n\" +\n",
    "                     f\"Epoch Index: {hp['num_epochs']}, \" +\n",
    "                     f\"latent_dim: {hp['latent_dim']}, \" +\n",
    "                     f\"loss_type: {hp['loss_type']}, \" +\n",
    "                     f\"learning_rate: {hp['learning_rate']}, \" +\n",
    "                     f\"weight_decay: {hp['weight_decay']}, \" +\n",
    "                     f\"alpha: {hp['alpha']}, \" +\n",
    "                     f\"beta: {hp['beta']}, \" +\n",
    "                     f\"theta: {hp['theta']}, \" +\n",
    "                     f\"lamda: {hp['lamda']}\")\n",
    "        fig.suptitle(title_str, y=1.02, fontsize=10)\n",
    "\n",
    "        for row in range(10):\n",
    "            for col in range(10):\n",
    "                image_idx = row * 10 + col\n",
    "                axes[row, col].imshow(generated_images[image_idx])\n",
    "                axes[row, col].axis('off')\n",
    "                if col == 0:  # If it's the first column, we'll add a label indicating the score\n",
    "                    axes[row, col].set_ylabel(f\"Score: {row}\", fontsize=12)\n",
    "\n",
    "        # Save the generated images per threshold as a PNG\n",
    "        gen_results_path = os.path.join(\"results\", f\"{folder}_generated_results.png\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(gen_results_path)\n",
    "        plt.show()\n",
    "\n",
    "# Display generated images by score\n",
    "display_generated_images_by_score(result_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6121fd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Call the function for each strategy\n",
    "generate_and_display_images_for_each_folder(strategy=\"ensemble\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc9e761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate images with probability-based scores\n",
    "def generate_images_with_probability(model, c_dim, hyperparameters):\n",
    "    model.eval()\n",
    "    all_samples = []\n",
    "\n",
    "    for threshold in range(10):  # for thresholds from 0 to 9\n",
    "        # Setting scores' probabilities based on the threshold\n",
    "        probabilities = [1/(10-threshold) if score >= threshold else 0 for score in range(10)]\n",
    "\n",
    "        # Sample a score based on the probabilities\n",
    "        sampled_score = np.random.choice(range(10), p=probabilities)\n",
    "\n",
    "        # Create condition vector with the sampled score\n",
    "        condition = torch.zeros(1, c_dim).to(device)\n",
    "        condition[:, 0] = sampled_score  # assuming 0 corresponds to Moira's score\n",
    "        condition[:, 1] = sampled_score  # assuming 1 corresponds to Ferdinando's score\n",
    "\n",
    "        # Generate image for the sampled score\n",
    "        z = torch.randn(1, int(hyperparameters[\"latent_dim\"])).to(device)\n",
    "        with torch.no_grad():\n",
    "            sample = model.decode(z, condition)\n",
    "\n",
    "        # Resize the image to 96x128\n",
    "        resized_sample = F.interpolate(sample, size=(96, 128), mode='bilinear', align_corners=False)\n",
    "        all_samples.append(resized_sample.squeeze(0))\n",
    "\n",
    "    return torch.stack(all_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01591d17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Call the function for each strategy\n",
    "generate_and_display_images_for_each_folder(strategy=\"probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afab5c66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def iterative_refinement(model, c_dim, start_score, end_score, hyperparameters):\n",
    "    model.eval()\n",
    "    # Generate initial image with start_score\n",
    "    z = torch.randn(1, int(hyperparameters[\"latent_dim\"])).to(device)  # Convert latent_dim to int\n",
    "    condition = torch.zeros(1, c_dim).to(device)\n",
    "    condition[:, 0] = start_score\n",
    "    condition[:, 1] = start_score\n",
    "\n",
    "    with torch.no_grad():\n",
    "        refined_sample = model.decode(z, condition)\n",
    "\n",
    "    # Iteratively refine the image\n",
    "    for score in range(start_score + 1, end_score + 1):\n",
    "        condition[:, 0] = score\n",
    "        condition[:, 1] = score\n",
    "\n",
    "        # Using the previously generated image as an input\n",
    "        refined_sample, _, _ = model(refined_sample, condition)\n",
    "\n",
    "    # Resize the image to 96x128\n",
    "    refined_sample_resized = F.interpolate(refined_sample, size=(96, 128), mode='bilinear', align_corners=False)\n",
    "\n",
    "    return refined_sample_resized.unsqueeze(0)  # Add a batch dimension\n",
    "\n",
    "# Call the function for each strategy\n",
    "generate_and_display_images_for_each_folder(strategy=\"refinement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b9ba04",
   "metadata": {},
   "source": [
    "### Hyperparameters search with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b861fb83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def hyperparameter_search(train_loader, val_loader, hyperparameters, n_trials ):\n",
    "    \n",
    "    trial_durations = []\n",
    "\n",
    "    def objective(trial):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Suggest values for the hyperparameters\n",
    "        beta = trial.suggest_float('beta', 1e-6, 1.0)\n",
    "        alpha = trial.suggest_float('alpha', 1e-2, 100)\n",
    "        theta = trial.suggest_float('theta', 1, 1000)\n",
    "        lamda = trial.suggest_float('lamda', 1, 1e10)\n",
    "\n",
    "        # Train the model for few epochs with the given hyperparameters\n",
    "        model, loss_dict = train_and_visualize_losses(cvae, train_loader, val_loader, optimizer, scheduler, \n",
    "                                                      beta=beta, hyperparameters, alpha, theta, lamda, image_show=False, debug=False)\n",
    "        \n",
    "        \n",
    "        # Compute the standard deviation of the individual losses\n",
    "        mse_loss = loss_dict['mse_loss']\n",
    "        perceptual_loss = loss_dict['perceptual_loss']\n",
    "        hist_loss = loss_dict['hist_loss']\n",
    "        current_std = np.std([alpha * mse_loss, theta * perceptual_loss, lamda * hist_loss])\n",
    "        \n",
    "        end_time = time.time()\n",
    "        duration = end_time - start_time\n",
    "        trial_durations.append(duration)\n",
    "        \n",
    "        avg_time_per_trial = sum(trial_durations) / len(trial_durations)\n",
    "        estimated_time_left = avg_time_per_trial * (n_trials - trial.number - 1)  # Assuming n_trials=10\n",
    "        \n",
    "        print(f\"Trial {trial.number} completed with STD: {current_std}. Estimated time remaining: {estimated_time_left/60:.2f} minutes\")\n",
    "        \n",
    "        return current_std\n",
    "\n",
    "    # Create a study object and specify the direction is 'minimize'.\n",
    "    sampler = optuna.samplers.TPESampler()\n",
    "    study = optuna.create_study(sampler=sampler, direction='minimize')\n",
    "    \n",
    "    # Optimize the study, the objective function is passed in as the first argument.\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    best_params = study.best_params\n",
    "\n",
    "    return best_params\n",
    "\n",
    "# Set the number of epochs in hyperparameters to 1\n",
    "hyperparameters['num_epochs'] = 1\n",
    "\n",
    "# Execute hyperparameter search\n",
    "best_params = hyperparameter_search(train_loader, val_loader, hyperparameters, n_trials = 1000 )\n",
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cb57c1",
   "metadata": {},
   "source": [
    "### Hyperparameters search with adaptive weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe96ee22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def adaptive_weights(losses):\n",
    "    \"\"\"Compute adaptive weights for the losses based on their magnitudes.\"\"\"\n",
    "    inverse_losses = [1.0 / (loss + 1e-5) for loss in losses]  # add a small value to prevent division by zero\n",
    "    sum_inverse_losses = sum(inverse_losses)\n",
    "    weights = [inv_loss / sum_inverse_losses for inv_loss in inverse_losses]\n",
    "    return weights\n",
    "\n",
    "def hyperparameter_search(train_loader, val_loader, hyperparameters, n_trials):\n",
    "    \n",
    "    trial_durations = []\n",
    "\n",
    "    def objective(trial):\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Suggest values for the hyperparameters\n",
    "        beta = trial.suggest_float('beta', 1e-6, 1.0)\n",
    "        alpha = trial.suggest_float('alpha', 1e-2, 10)\n",
    "        theta = trial.suggest_float('theta', 1, 100)\n",
    "        lamda = trial.suggest_float('lamda', 4*1e9, 7*1e9)\n",
    "\n",
    "        # Train the model for few epochs with the given hyperparameters\n",
    "        model, loss_dict = train_and_visualize_losses(cvae, train_loader, val_loader, optimizer, scheduler, \n",
    "                                                      hyperparameters, beta, alpha, theta, lamda, image_show=False, debug=False)\n",
    "        \n",
    "        # Multiply each loss by its respective coefficient\n",
    "        mse_loss = alpha * loss_dict['mse_loss']\n",
    "        perceptual_loss = theta * loss_dict['perceptual_loss']\n",
    "        hist_loss = lamda * loss_dict['hist_loss']\n",
    "\n",
    "        weights = adaptive_weights([mse_loss, perceptual_loss, hist_loss])\n",
    "        weighted_std = np.std([mse_loss * weights[0], perceptual_loss * weights[1], hist_loss * weights[2]])\n",
    "        \n",
    "        end_time = time.time()\n",
    "        duration = end_time - start_time\n",
    "        trial_durations.append(duration)\n",
    "        \n",
    "        avg_time_per_trial = sum(trial_durations) / len(trial_durations)\n",
    "        estimated_time_left = avg_time_per_trial * (n_trials - trial.number - 1)\n",
    "        \n",
    "        print(f\"Trial {trial.number} completed with Weighted STD: {weighted_std}. Estimated time remaining: {estimated_time_left/60:.2f} minutes\")\n",
    "        \n",
    "        return weighted_std\n",
    "\n",
    "    # Create a study object and specify the direction is 'minimize'.\n",
    "    sampler = optuna.samplers.TPESampler()\n",
    "    study = optuna.create_study(sampler=sampler, direction='minimize')\n",
    "    \n",
    "    # Optimize the study, the objective function is passed in as the first argument.\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    best_params = study.best_params\n",
    "\n",
    "    return best_params\n",
    "\n",
    "# Set the number of epochs in hyperparameters to 1\n",
    "hyperparameters['num_epochs'] = 1\n",
    "\n",
    "# Execute hyperparameter search\n",
    "best_params = hyperparameter_search(train_loader, val_loader, hyperparameters, n_trials = 10)\n",
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe86a41",
   "metadata": {},
   "source": [
    "### Hyperparameters search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc464328",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def hyperparameter_search(train_loader, val_loader, hyperparameters):\n",
    "    # Define search space\n",
    "    alpha_values = [1e-3, 1e-2, 1e-1]\n",
    "    theta_values = [1, 5, 10, 100]\n",
    "    lamda_values = [1, 1e3, 1e5, 1e7, 1e10]\n",
    "\n",
    "    best_std = float('inf')  # We aim to minimize the standard deviation\n",
    "    best_hyperparameters = {}\n",
    "\n",
    "    total_combinations = len(alpha_values) * len(theta_values) * len(lamda_values)\n",
    "    iteration_time_list = []  # Store the time taken for each iteration\n",
    "\n",
    "    # For each combination of hyperparameters\n",
    "    for index, (alpha, theta, lamda) in enumerate(product(alpha_values, theta_values, lamda_values)):\n",
    "        print(f\"Training with alpha: {alpha}, theta: {theta}, lamda: {lamda}\")\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Modify the loss function to use the current hyperparameters\n",
    "        def modified_combined_cvae_loss(recon_x, x, mu, logvar, beta):\n",
    "            return combined_cvae_loss(recon_x, x, mu, logvar, beta, alpha, theta, lamda)\n",
    "\n",
    "        # Train the model for few epochs\n",
    "        model, loss_dict = train_and_visualize_losses(cvae, train_loader, val_loader, optimizer, scheduler, \n",
    "                                                 hyperparameters, alpha, theta, lamda, image_show=False, debug=False)\n",
    "        \n",
    "\n",
    "        end_time = time.time()\n",
    "        iteration_time = end_time - start_time\n",
    "        iteration_time_list.append(iteration_time)\n",
    "\n",
    "        # Compute the standard deviation of the individual losses\n",
    "        mse_loss = loss_dict['mse_loss']\n",
    "        perceptual_loss = loss_dict['perceptual_loss']\n",
    "        hist_loss = loss_dict['hist_loss']\n",
    "\n",
    "        current_std = np.std([mse_loss, perceptual_loss, hist_loss])\n",
    "\n",
    "        # Update the best hyperparameters if current standard deviation is lower\n",
    "        if current_std < best_std:\n",
    "            best_std = current_std\n",
    "            best_hyperparameters = {\n",
    "                'alpha': alpha,\n",
    "                'theta': theta,\n",
    "                'lamda': lamda\n",
    "            }\n",
    "\n",
    "        avg_time_per_iteration = sum(iteration_time_list) / len(iteration_time_list)\n",
    "        estimated_time_left = avg_time_per_iteration * (total_combinations - index - 1)\n",
    "        print(f\"Estimated time remaining: {estimated_time_left/60:.2f} minutes\")\n",
    "\n",
    "    return best_hyperparameters\n",
    "\n",
    "# Set the number of epochs in hyperparameters to 2\n",
    "hyperparameters['num_epochs'] = 2\n",
    "\n",
    "# Execute hyperparameter search\n",
    "best_params = hyperparameter_search(train_loader, val_loader, hyperparameters)\n",
    "best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc149d95",
   "metadata": {},
   "source": [
    "## Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f490a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def save_best_model(study, trial):\n",
    "    if trial.value == study.best_value:\n",
    "        best_model = trial.user_attrs[\"model\"]\n",
    "        torch.save(best_model.state_dict(), f\"best_model_trial_{trial.number}.pth\")\n",
    "\n",
    "def objective(trial):\n",
    "    # 1. Define the hyperparameters to tune with steps\n",
    "    latent_dim = trial.suggest_int(\"latent_dim\", 128, 512, step=128)\n",
    "    beta = trial.suggest_float(\"beta\", 0.0001, 10.0, log=True)\n",
    "    loss_type = trial.suggest_categorical(\"loss_type\", [\"MSE\"])\n",
    "    \n",
    "    hyperparameters = {\n",
    "        \"num_epochs\": 101,\n",
    "        \"patience\": 50,\n",
    "        \"beta\": beta,  \n",
    "        \"save_interval\": 1,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"latent_dim\": latent_dim,\n",
    "        \"condition_dim\": 2,\n",
    "        \"loss_type\": \"MSE\"\n",
    "    }\n",
    "\n",
    "    # 2. Initialize model, optimizer etc. with chosen hyperparameters\n",
    "    model = CVAE_ResNet18_DualEmbedding(conditional_dim=10, latent_dim=latent_dim, debug=False).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "\n",
    "    # 3. Train the model\n",
    "    trained_model, best_val_loss = train_and_visualize_losses(model, train_loader, val_loader, \n",
    "                                           optimizer, scheduler, hyperparameters, image_show=True, debug=False)\n",
    "    \n",
    "    # 4. Retrieve the best validation loss from the training\n",
    "    trial.set_user_attr(\"model\", trained_model)\n",
    "    \n",
    "    return best_val_loss\n",
    "\n",
    "# Create a study object and specify the direction is 'minimize'.\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "\n",
    "# Optimize the study\n",
    "study.optimize(objective, n_trials=50, callbacks=[save_best_model])\n",
    "\n",
    "# Results\n",
    "print(\"Number of completed trials: \", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"Value: \", trial.value)\n",
    "print(\"Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "in5310",
   "language": "python",
   "name": "in5310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
